"""
AI Explainability Engine - Enterprise-Grade AI Transparency
Provides explainable AI decisions for enterprise compliance and trust
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import json
from dataclasses import dataclass, asdict
from enum import Enum
import asyncio

class ExplanationType(Enum):
    FEATURE_IMPORTANCE = "feature_importance"
    DECISION_PATH = "decision_path"
    COUNTERFACTUAL = "counterfactual"
    CONFIDENCE_INTERVALS = "confidence_intervals"
    BIAS_ANALYSIS = "bias_analysis"

@dataclass
class AIDecision:
    decision_id: str
    model_name: str
    prediction: Any
    confidence: float
    input_features: Dict[str, Any]
    timestamp: datetime
    user_id: str
    business_context: str

@dataclass
class Explanation:
    decision_id: str
    explanation_type: ExplanationType
    content: Dict[str, Any]
    confidence: float
    human_readable: str
    technical_details: Dict[str, Any]

class AIExplainabilityEngine:
    """Enterprise AI explainability and interpretability engine"""
    
    def __init__(self):
        self.explanation_cache = {}
        self.model_metadata = {}
        self.bias_detector = BiasDetector()
        self.fairness_analyzer = FairnessAnalyzer()
        
    async def explain_decision(self, decision: AIDecision, 
                              explanation_types: List[ExplanationType] = None) -> Dict[str, Any]:
        """Generate comprehensive explanations for AI decisions"""
        try:
            if explanation_types is None:
                explanation_types = [
                    ExplanationType.FEATURE_IMPORTANCE,
                    ExplanationType.DECISION_PATH,
                    ExplanationType.CONFIDENCE_INTERVALS
                ]
            
            explanations = {}
            
            for exp_type in explanation_types:
                explanation = await self._generate_explanation(decision, exp_type)
                explanations[exp_type.value] = explanation
            
            # Generate executive summary
            executive_summary = await self._generate_executive_summary(decision, explanations)
            
            # Perform bias analysis
            bias_analysis = await self.bias_detector.analyze_decision(decision)
            
            # Generate recommendations
            recommendations = await self._generate_recommendations(decision, explanations, bias_analysis)
            
            return {
                "decision_id": decision.decision_id,
                "model_name": decision.model_name,
                "prediction": decision.prediction,
                "confidence": decision.confidence,
                "explanations": explanations,
                "executive_summary": executive_summary,
                "bias_analysis": bias_analysis,
                "recommendations": recommendations,
                "compliance_metadata": {
                    "gdpr_compliant": True,
                    "ai_act_compliant": True,
                    "audit_trail": f"explanation_generated_{datetime.now().isoformat()}"
                },
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {"error": f"Failed to generate explanation: {str(e)}"}
    
    async def _generate_explanation(self, decision: AIDecision, 
                                   exp_type: ExplanationType) -> Explanation:
        """Generate specific type of explanation"""
        
        if exp_type == ExplanationType.FEATURE_IMPORTANCE:
            return await self._explain_feature_importance(decision)
        elif exp_type == ExplanationType.DECISION_PATH:
            return await self._explain_decision_path(decision)
        elif exp_type == ExplanationType.COUNTERFACTUAL:
            return await self._explain_counterfactual(decision)
        elif exp_type == ExplanationType.CONFIDENCE_INTERVALS:
            return await self._explain_confidence(decision)
        else:
            return Explanation(
                decision_id=decision.decision_id,
                explanation_type=exp_type,
                content={},
                confidence=0.0,
                human_readable="Explanation type not supported",
                technical_details={}
            )
    
    async def _explain_feature_importance(self, decision: AIDecision) -> Explanation:
        """Explain which features were most important for the decision"""
        
        # Calculate feature importance (SHAP-like analysis)
        feature_impacts = {}
        total_impact = 0
        
        for feature, value in decision.input_features.items():
            # Simulate feature importance calculation
            impact = self._calculate_feature_impact(feature, value, decision.business_context)
            feature_impacts[feature] = {
                "value": value,
                "impact_score": impact,
                "impact_direction": "positive" if impact > 0 else "negative",
                "importance_rank": 0  # Will be calculated below
            }
            total_impact += abs(impact)
        
        # Rank features by importance
        ranked_features = sorted(feature_impacts.items(), 
                               key=lambda x: abs(x[1]["impact_score"]), reverse=True)
        
        for i, (feature, data) in enumerate(ranked_features):
            feature_impacts[feature]["importance_rank"] = i + 1
            feature_impacts[feature]["relative_importance"] = abs(data["impact_score"]) / total_impact if total_impact > 0 else 0
        
        # Generate human-readable explanation
        top_features = ranked_features[:3]
        human_readable = f"The top 3 factors influencing this decision were: "
        human_readable += ", ".join([
            f"{feature} (impact: {data['impact_score']:.2f})" 
            for feature, data in top_features
        ])
        
        return Explanation(
            decision_id=decision.decision_id,
            explanation_type=ExplanationType.FEATURE_IMPORTANCE,
            content={
                "feature_impacts": feature_impacts,
                "top_features": dict(top_features[:5]),
                "total_features_analyzed": len(feature_impacts)
            },
            confidence=0.85,
            human_readable=human_readable,
            technical_details={
                "method": "SHAP-inspired feature attribution",
                "normalization": "relative_to_total_impact",
                "calculation_timestamp": datetime.now().isoformat()
            }
        )
    
    async def _explain_decision_path(self, decision: AIDecision) -> Explanation:
        """Explain the decision-making path through the model"""
        
        # Simulate decision tree path
        decision_path = [
            {
                "step": 1,
                "condition": f"cpu_usage <= 80%",
                "value": decision.input_features.get("cpu_usage", 45),
                "result": "condition_met",
                "confidence": 0.9
            },
            {
                "step": 2,
                "condition": f"memory_usage <= 85%",
                "value": decision.input_features.get("memory_usage", 60),
                "result": "condition_met",
                "confidence": 0.85
            },
            {
                "step": 3,
                "condition": f"cost_trend <= threshold",
                "value": decision.input_features.get("cost_trend", 0.05),
                "result": "condition_met",
                "confidence": 0.8
            }
        ]
        
        # Generate path summary
        path_summary = " â†’ ".join([
            f"Step {step['step']}: {step['condition']} ({step['result']})"
            for step in decision_path
        ])
        
        human_readable = f"Decision path: {path_summary}. All conditions were met leading to prediction: {decision.prediction}"
        
        return Explanation(
            decision_id=decision.decision_id,
            explanation_type=ExplanationType.DECISION_PATH,
            content={
                "decision_path": decision_path,
                "path_summary": path_summary,
                "final_prediction": decision.prediction,
                "path_confidence": np.mean([step["confidence"] for step in decision_path])
            },
            confidence=0.82,
            human_readable=human_readable,
            technical_details={
                "model_type": "ensemble_decision_tree",
                "path_length": len(decision_path),
                "branching_factor": "average_3.2"
            }
        )
    
    async def _explain_counterfactual(self, decision: AIDecision) -> Explanation:
        """Generate counterfactual explanations - what would change the decision"""
        
        counterfactuals = []
        
        # Generate counterfactual scenarios
        for feature, current_value in decision.input_features.items():
            if isinstance(current_value, (int, float)):
                # Generate what-if scenarios
                scenarios = [
                    {"change": f"Increase {feature} by 20%", "new_value": current_value * 1.2},
                    {"change": f"Decrease {feature} by 20%", "new_value": current_value * 0.8},
                ]
                
                for scenario in scenarios:
                    predicted_outcome = await self._predict_counterfactual(
                        decision, feature, scenario["new_value"]
                    )
                    
                    counterfactuals.append({
                        "feature": feature,
                        "current_value": current_value,
                        "counterfactual_value": scenario["new_value"],
                        "change_description": scenario["change"],
                        "predicted_outcome": predicted_outcome,
                        "outcome_change": predicted_outcome != decision.prediction,
                        "confidence": 0.75
                    })
        
        # Find minimal changes needed
        minimal_changes = [cf for cf in counterfactuals if cf["outcome_change"]][:3]
        
        human_readable = "To change this decision, you could: " + "; or ".join([
            cf["change_description"] for cf in minimal_changes
        ]) if minimal_changes else "No simple changes would alter this decision"
        
        return Explanation(
            decision_id=decision.decision_id,
            explanation_type=ExplanationType.COUNTERFACTUAL,
            content={
                "counterfactuals": counterfactuals,
                "minimal_changes": minimal_changes,
                "total_scenarios": len(counterfactuals)
            },
            confidence=0.78,
            human_readable=human_readable,
            technical_details={
                "method": "feature_perturbation",
                "perturbation_range": "Â±20%",
                "scenarios_generated": len(counterfactuals)
            }
        )
    
    async def _explain_confidence(self, decision: AIDecision) -> Explanation:
        """Explain confidence intervals and uncertainty"""
        
        # Calculate confidence components
        data_quality_score = 0.9  # Based on input data completeness
        model_certainty = decision.confidence
        historical_accuracy = 0.85  # Based on model's historical performance
        
        # Calculate uncertainty sources
        uncertainty_sources = {
            "data_quality": 1.0 - data_quality_score,
            "model_uncertainty": 1.0 - model_certainty,
            "historical_variance": 1.0 - historical_accuracy,
            "feature_correlation": 0.1  # Estimated feature correlation uncertainty
        }
        
        # Calculate overall confidence interval
        total_uncertainty = np.sqrt(sum(u**2 for u in uncertainty_sources.values()))
        confidence_interval = {
            "lower_bound": max(0, decision.confidence - total_uncertainty),
            "upper_bound": min(1, decision.confidence + total_uncertainty),
            "interval_width": 2 * total_uncertainty
        }
        
        # Determine confidence level
        if decision.confidence > 0.9:
            confidence_level = "Very High"
        elif decision.confidence > 0.7:
            confidence_level = "High"
        elif decision.confidence > 0.5:
            confidence_level = "Medium"
        else:
            confidence_level = "Low"
        
        human_readable = f"Confidence level: {confidence_level} ({decision.confidence:.1%}). " \
                        f"The prediction interval ranges from {confidence_interval['lower_bound']:.1%} " \
                        f"to {confidence_interval['upper_bound']:.1%}."
        
        return Explanation(
            decision_id=decision.decision_id,
            explanation_type=ExplanationType.CONFIDENCE_INTERVALS,
            content={
                "confidence_interval": confidence_interval,
                "uncertainty_sources": uncertainty_sources,
                "confidence_level": confidence_level,
                "reliability_metrics": {
                    "data_quality_score": data_quality_score,
                    "model_certainty": model_certainty,
                    "historical_accuracy": historical_accuracy
                }
            },
            confidence=0.88,
            human_readable=human_readable,
            technical_details={
                "method": "uncertainty_propagation",
                "interval_type": "confidence_interval_95%",
                "uncertainty_calculation": "root_sum_squares"
            }
        )
    
    def _calculate_feature_impact(self, feature: str, value: Any, context: str) -> float:
        """Calculate the impact of a feature on the decision"""
        # Simulate SHAP-like feature attribution
        
        feature_weights = {
            "cpu_usage": 0.25,
            "memory_usage": 0.20,
            "cost_trend": 0.30,
            "network_traffic": 0.15,
            "error_rate": 0.35,
            "user_count": 0.10
        }
        
        base_weight = feature_weights.get(feature, 0.1)
        
        # Adjust based on value magnitude and context
        if isinstance(value, (int, float)):
            if feature in ["cpu_usage", "memory_usage"] and value > 80:
                return base_weight * 1.5  # High utilization increases impact
            elif feature == "error_rate" and value > 0.05:
                return base_weight * 2.0  # High error rate is critical
            elif feature == "cost_trend" and value > 0.1:
                return base_weight * 1.3  # Cost increases are important
        
        return base_weight * np.random.uniform(0.8, 1.2)  # Add some realistic variance
    
    async def _predict_counterfactual(self, decision: AIDecision, 
                                    changed_feature: str, new_value: Any) -> Any:
        """Predict outcome if one feature was changed"""
        # Simulate model prediction with changed feature
        # In reality, this would use the actual model
        
        if changed_feature == "cpu_usage":
            if new_value > 90:
                return "scale_up_required"
            elif new_value < 30:
                return "scale_down_opportunity"
        elif changed_feature == "cost_trend":
            if new_value > 0.2:
                return "cost_optimization_needed"
        
        return decision.prediction  # No change in most cases
    
    async def _generate_executive_summary(self, decision: AIDecision, 
                                        explanations: Dict[str, Any]) -> str:
        """Generate executive summary of the AI decision"""
        
        summary_parts = []
        
        # Add prediction summary
        summary_parts.append(f"The AI model '{decision.model_name}' predicted '{decision.prediction}' with {decision.confidence:.1%} confidence.")
        
        # Add key factors
        if "feature_importance" in explanations:
            top_features = list(explanations["feature_importance"].content["top_features"].keys())[:2]
            summary_parts.append(f"Key factors: {', '.join(top_features)}.")
        
        # Add confidence assessment
        if decision.confidence > 0.8:
            summary_parts.append("This is a high-confidence prediction suitable for automated action.")
        elif decision.confidence > 0.6:
            summary_parts.append("This is a medium-confidence prediction that may benefit from human review.")
        else:
            summary_parts.append("This is a low-confidence prediction requiring human oversight.")
        
        # Add business impact
        if decision.business_context:
            summary_parts.append(f"Business context: {decision.business_context}.")
        
        return " ".join(summary_parts)
    
    async def _generate_recommendations(self, decision: AIDecision, 
                                      explanations: Dict[str, Any], 
                                      bias_analysis: Dict[str, Any]) -> List[Dict[str, str]]:
        """Generate actionable recommendations based on explanations"""
        
        recommendations = []
        
        # Confidence-based recommendations
        if decision.confidence < 0.7:
            recommendations.append({
                "type": "data_quality",
                "priority": "high",
                "action": "Collect more training data to improve model confidence",
                "rationale": f"Current confidence is only {decision.confidence:.1%}"
            })
        
        # Bias-based recommendations
        if bias_analysis.get("bias_detected"):
            recommendations.append({
                "type": "fairness",
                "priority": "critical",
                "action": "Review model for bias and implement fairness constraints",
                "rationale": "Potential bias detected in decision-making process"
            })
        
        # Feature-based recommendations
        if "feature_importance" in explanations:
            top_feature = list(explanations["feature_importance"].content["top_features"].keys())[0]
            recommendations.append({
                "type": "monitoring",
                "priority": "medium",
                "action": f"Set up enhanced monitoring for {top_feature}",
                "rationale": f"{top_feature} is the most influential factor in this decision"
            })
        
        return recommendations

class BiasDetector:
    """Detect bias in AI decisions"""
    
    async def analyze_decision(self, decision: AIDecision) -> Dict[str, Any]:
        """Analyze decision for potential bias"""
        
        bias_indicators = {
            "demographic_parity": await self._check_demographic_parity(decision),
            "equalized_odds": await self._check_equalized_odds(decision),
            "individual_fairness": await self._check_individual_fairness(decision)
        }
        
        # Overall bias assessment
        bias_scores = [indicator["bias_score"] for indicator in bias_indicators.values()]
        overall_bias_score = np.mean(bias_scores)
        bias_detected = overall_bias_score > 0.3
        
        return {
            "bias_detected": bias_detected,
            "overall_bias_score": overall_bias_score,
            "bias_indicators": bias_indicators,
            "risk_level": "high" if overall_bias_score > 0.5 else "medium" if overall_bias_score > 0.3 else "low",
            "recommendations": self._generate_bias_recommendations(bias_indicators) if bias_detected else []
        }
    
    async def _check_demographic_parity(self, decision: AIDecision) -> Dict[str, Any]:
        """Check for demographic parity violations"""
        # Mock implementation - would use actual demographic data
        return {
            "bias_score": 0.1,  # Low bias
            "description": "Demographic parity check",
            "details": "No significant disparity detected across user groups"
        }
    
    async def _check_equalized_odds(self, decision: AIDecision) -> Dict[str, Any]:
        """Check for equalized odds violations"""
        return {
            "bias_score": 0.05,  # Very low bias
            "description": "Equalized odds check",
            "details": "Similar accuracy rates across different user segments"
        }
    
    async def _check_individual_fairness(self, decision: AIDecision) -> Dict[str, Any]:
        """Check for individual fairness violations"""
        return {
            "bias_score": 0.15,  # Low bias
            "description": "Individual fairness check",
            "details": "Similar individuals receive similar predictions"
        }
    
    def _generate_bias_recommendations(self, bias_indicators: Dict[str, Any]) -> List[str]:
        """Generate recommendations to address bias"""
        recommendations = []
        
        for indicator_name, indicator in bias_indicators.items():
            if indicator["bias_score"] > 0.3:
                recommendations.append(f"Address {indicator_name} bias through data balancing")
                recommendations.append(f"Implement fairness constraints for {indicator_name}")
        
        return recommendations

class FairnessAnalyzer:
    """Analyze AI fairness across different dimensions"""
    
    async def analyze_fairness(self, decisions: List[AIDecision]) -> Dict[str, Any]:
        """Comprehensive fairness analysis across multiple decisions"""
        
        fairness_metrics = {
            "statistical_parity": await self._calculate_statistical_parity(decisions),
            "equal_opportunity": await self._calculate_equal_opportunity(decisions),
            "calibration": await self._calculate_calibration(decisions),
            "counterfactual_fairness": await self._calculate_counterfactual_fairness(decisions)
        }
        
        overall_fairness_score = np.mean([metric["score"] for metric in fairness_metrics.values()])
        
        return {
            "overall_fairness_score": overall_fairness_score,
            "fairness_level": self._determine_fairness_level(overall_fairness_score),
            "fairness_metrics": fairness_metrics,
            "recommendations": self._generate_fairness_recommendations(fairness_metrics),
            "compliance_status": {
                "ai_act_compliant": overall_fairness_score > 0.7,
                "ethical_ai_guidelines": overall_fairness_score > 0.8,
                "industry_standards": overall_fairness_score > 0.75
            }
        }
    
    async def _calculate_statistical_parity(self, decisions: List[AIDecision]) -> Dict[str, Any]:
        """Calculate statistical parity metric"""
        # Mock calculation
        return {
            "score": 0.85,
            "description": "Equal selection rates across groups",
            "interpretation": "Good statistical parity maintained"
        }
    
    async def _calculate_equal_opportunity(self, decisions: List[AIDecision]) -> Dict[str, Any]:
        """Calculate equal opportunity metric"""
        return {
            "score": 0.82,
            "description": "Equal true positive rates across groups",
            "interpretation": "Acceptable equal opportunity performance"
        }
    
    async def _calculate_calibration(self, decisions: List[AIDecision]) -> Dict[str, Any]:
        """Calculate calibration metric"""
        return {
            "score": 0.88,
            "description": "Prediction probabilities match actual outcomes",
            "interpretation": "Well-calibrated predictions across groups"
        }
    
    async def _calculate_counterfactual_fairness(self, decisions: List[AIDecision]) -> Dict[str, Any]:
        """Calculate counterfactual fairness metric"""
        return {
            "score": 0.79,
            "description": "Decisions remain same in counterfactual world",
            "interpretation": "Good counterfactual fairness"
        }
    
    def _determine_fairness_level(self, score: float) -> str:
        """Determine fairness level based on score"""
        if score > 0.9:
            return "Excellent"
        elif score > 0.8:
            return "Good"
        elif score > 0.7:
            return "Acceptable"
        elif score > 0.6:
            return "Needs Improvement"
        else:
            return "Poor"
    
    def _generate_fairness_recommendations(self, metrics: Dict[str, Any]) -> List[str]:
        """Generate fairness improvement recommendations"""
        recommendations = []
        
        for metric_name, metric in metrics.items():
            if metric["score"] < 0.8:
                recommendations.append(f"Improve {metric_name} through targeted interventions")
        
        if not recommendations:
            recommendations.append("Maintain current fairness standards through regular monitoring")
        
        return recommendations

# AI Model Performance Optimizer
class AIModelOptimizer:
    """Optimize AI model performance and efficiency"""
    
    def __init__(self):
        self.performance_metrics = {}
        self.optimization_history = []
    
    async def optimize_model_performance(self, model_name: str, 
                                       performance_data: Dict[str, Any]) -> Dict[str, Any]:
        """Optimize model performance based on real-world metrics"""
        
        try:
            # Analyze current performance
            current_metrics = await self._analyze_current_performance(model_name, performance_data)
            
            # Identify optimization opportunities
            opportunities = await self._identify_optimization_opportunities(current_metrics)
            
            # Generate optimization plan
            optimization_plan = await self._generate_optimization_plan(opportunities)
            
            # Estimate improvement potential
            improvement_estimates = await self._estimate_improvements(optimization_plan)
            
            return {
                "model_name": model_name,
                "current_performance": current_metrics,
                "optimization_opportunities": opportunities,
                "optimization_plan": optimization_plan,
                "estimated_improvements": improvement_estimates,
                "implementation_priority": self._prioritize_optimizations(opportunities),
                "resource_requirements": await self._estimate_resources(optimization_plan),
                "timeline": await self._estimate_timeline(optimization_plan),
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {"error": f"Optimization analysis failed: {str(e)}"}
    
    async def _analyze_current_performance(self, model_name: str, 
                                         performance_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze current model performance metrics"""
        
        return {
            "accuracy": performance_data.get("accuracy", 0.85),
            "latency_ms": performance_data.get("latency_ms", 120),
            "throughput_qps": performance_data.get("throughput_qps", 50),
            "memory_usage_mb": performance_data.get("memory_usage_mb", 512),
            "cpu_utilization": performance_data.get("cpu_utilization", 0.45),
            "cost_per_prediction": performance_data.get("cost_per_prediction", 0.001),
            "model_size_mb": performance_data.get("model_size_mb", 100),
            "inference_efficiency": performance_data.get("throughput_qps", 50) / performance_data.get("latency_ms", 120) * 1000
        }
    
    async def _identify_optimization_opportunities(self, metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Identify specific optimization opportunities"""
        
        opportunities = []
        
        # Latency optimization
        if metrics["latency_ms"] > 100:
            opportunities.append({
                "type": "latency_optimization",
                "current_value": metrics["latency_ms"],
                "target_value": metrics["latency_ms"] * 0.7,
                "potential_improvement": "30% latency reduction",
                "methods": ["model_quantization", "batch_processing", "caching"],
                "impact": "high",
                "effort": "medium"
            })
        
        # Accuracy improvement
        if metrics["accuracy"] < 0.90:
            opportunities.append({
                "type": "accuracy_improvement",
                "current_value": metrics["accuracy"],
                "target_value": min(0.95, metrics["accuracy"] + 0.05),
                "potential_improvement": "5% accuracy increase",
                "methods": ["ensemble_methods", "feature_engineering", "hyperparameter_tuning"],
                "impact": "high",
                "effort": "high"
            })
        
        # Cost optimization
        if metrics["cost_per_prediction"] > 0.0005:
            opportunities.append({
                "type": "cost_optimization",
                "current_value": metrics["cost_per_prediction"],
                "target_value": metrics["cost_per_prediction"] * 0.6,
                "potential_improvement": "40% cost reduction",
                "methods": ["model_compression", "efficient_inference", "spot_instances"],
                "impact": "medium",
                "effort": "low"
            })
        
        # Memory optimization
        if metrics["memory_usage_mb"] > 400:
            opportunities.append({
                "type": "memory_optimization",
                "current_value": metrics["memory_usage_mb"],
                "target_value": metrics["memory_usage_mb"] * 0.8,
                "potential_improvement": "20% memory reduction",
                "methods": ["model_pruning", "weight_sharing", "dynamic_loading"],
                "impact": "medium",
                "effort": "medium"
            })
        
        return opportunities
    
    async def _generate_optimization_plan(self, opportunities: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate detailed optimization implementation plan"""
        
        # Sort by impact and effort
        prioritized_opportunities = sorted(opportunities, 
                                         key=lambda x: (x["impact"], -self._effort_score(x["effort"])))
        
        phases = []
        
        # Phase 1: High impact, low effort
        phase1 = [opp for opp in prioritized_opportunities 
                 if opp["impact"] == "high" and opp["effort"] in ["low", "medium"]]
        if phase1:
            phases.append({
                "phase": 1,
                "name": "Quick Wins",
                "optimizations": phase1,
                "duration_weeks": 2,
                "expected_impact": "immediate_improvement"
            })
        
        # Phase 2: High impact, high effort
        phase2 = [opp for opp in prioritized_opportunities 
                 if opp["impact"] == "high" and opp["effort"] == "high"]
        if phase2:
            phases.append({
                "phase": 2,
                "name": "Major Improvements",
                "optimizations": phase2,
                "duration_weeks": 6,
                "expected_impact": "significant_improvement"
            })
        
        # Phase 3: Medium impact optimizations
        phase3 = [opp for opp in prioritized_opportunities if opp["impact"] == "medium"]
        if phase3:
            phases.append({
                "phase": 3,
                "name": "Incremental Gains",
                "optimizations": phase3,
                "duration_weeks": 4,
                "expected_impact": "moderate_improvement"
            })
        
        return {
            "phases": phases,
            "total_duration_weeks": sum(phase["duration_weeks"] for phase in phases),
            "implementation_strategy": "phased_approach",
            "risk_mitigation": [
                "A/B testing for each optimization",
                "Gradual rollout with monitoring",
                "Rollback plan for each phase"
            ]
        }
    
    def _effort_score(self, effort: str) -> int:
        """Convert effort level to numeric score"""
        return {"low": 1, "medium": 2, "high": 3}.get(effort, 2)
    
    async def _estimate_improvements(self, optimization_plan: Dict[str, Any]) -> Dict[str, Any]:
        """Estimate overall improvements from optimization plan"""
        
        cumulative_improvements = {
            "latency_improvement": 0,
            "accuracy_improvement": 0,
            "cost_reduction": 0,
            "memory_reduction": 0,
            "throughput_increase": 0
        }
        
        for phase in optimization_plan["phases"]:
            for optimization in phase["optimizations"]:
                opt_type = optimization["type"]
                
                if opt_type == "latency_optimization":
                    cumulative_improvements["latency_improvement"] += 0.3
                elif opt_type == "accuracy_improvement":
                    cumulative_improvements["accuracy_improvement"] += 0.05
                elif opt_type == "cost_optimization":
                    cumulative_improvements["cost_reduction"] += 0.4
                elif opt_type == "memory_optimization":
                    cumulative_improvements["memory_reduction"] += 0.2
        
        # Calculate business value
        business_value = {
            "annual_cost_savings": cumulative_improvements["cost_reduction"] * 50000,  # $50k baseline
            "performance_value": cumulative_improvements["latency_improvement"] * 100000,  # User experience value
            "accuracy_value": cumulative_improvements["accuracy_improvement"] * 200000,  # Business impact
            "total_estimated_value": 0
        }
        business_value["total_estimated_value"] = sum(business_value.values()) - business_value["total_estimated_value"]
        
        return {
            "performance_improvements": cumulative_improvements,
            "business_value": business_value,
            "roi_estimate": business_value["total_estimated_value"] / 100000,  # Assuming $100k investment
            "payback_period_months": 12 / max(1, business_value["total_estimated_value"] / 100000)
        }
    
    def _prioritize_optimizations(self, opportunities: List[Dict[str, Any]]) -> List[Dict[str, str]]:
        """Prioritize optimizations by impact and effort"""
        
        priority_matrix = []
        
        for opp in opportunities:
            if opp["impact"] == "high" and opp["effort"] == "low":
                priority = "P0 - Critical"
            elif opp["impact"] == "high" and opp["effort"] == "medium":
                priority = "P1 - High"
            elif opp["impact"] == "high" and opp["effort"] == "high":
                priority = "P2 - Medium"
            else:
                priority = "P3 - Low"
            
            priority_matrix.append({
                "optimization_type": opp["type"],
                "priority": priority,
                "rationale": f"{opp['impact']} impact, {opp['effort']} effort"
            })
        
        return sorted(priority_matrix, key=lambda x: x["priority"])
    
    async def _estimate_resources(self, optimization_plan: Dict[str, Any]) -> Dict[str, Any]:
        """Estimate resource requirements for optimization"""
        
        total_phases = len(optimization_plan["phases"])
        
        return {
            "engineering_weeks": optimization_plan["total_duration_weeks"] * 0.8,
            "data_science_weeks": optimization_plan["total_duration_weeks"] * 0.6,
            "infrastructure_cost": total_phases * 5000,  # $5k per phase
            "compute_resources": {
                "cpu_hours": optimization_plan["total_duration_weeks"] * 100,
                "gpu_hours": optimization_plan["total_duration_weeks"] * 50,
                "storage_gb": 1000
            },
            "estimated_total_cost": optimization_plan["total_duration_weeks"] * 15000
        }
    
    async def _estimate_timeline(self, optimization_plan: Dict[str, Any]) -> Dict[str, Any]:
        """Estimate implementation timeline"""
        
        start_date = datetime.now()
        
        phase_timelines = []
        current_date = start_date
        
        for phase in optimization_plan["phases"]:
            end_date = current_date + timedelta(weeks=phase["duration_weeks"])
            
            phase_timelines.append({
                "phase": phase["phase"],
                "name": phase["name"],
                "start_date": current_date.isoformat(),
                "end_date": end_date.isoformat(),
                "duration_weeks": phase["duration_weeks"],
                "milestones": [
                    f"Week {i+1}: {milestone}" for i, milestone in enumerate([
                        "Planning and setup",
                        "Implementation",
                        "Testing and validation",
                        "Deployment and monitoring"
                    ][:phase["duration_weeks"]])
                ]
            })
            
            current_date = end_date
        
        return {
            "total_timeline": {
                "start_date": start_date.isoformat(),
                "end_date": current_date.isoformat(),
                "total_duration_weeks": optimization_plan["total_duration_weeks"]
            },
            "phase_timelines": phase_timelines,
            "critical_path": "All phases are sequential",
            "risk_buffer": "20% additional time for unforeseen issues"
        }

# Global instances
explainability_engine = AIExplainabilityEngine()
model_optimizer = AIModelOptimizer()